{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 11:26:10.634331: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-19 11:26:10.643891: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739944570.654913    8815 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739944570.658395    8815 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-19 11:26:10.670146: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/skullboxml/minor_project/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset, load_dataset\n",
    "from evaluate import load\n",
    "import numpy as np\n",
    "\n",
    "from typing import List, Dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 11:26:56.316751: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "import dataloading\n",
    "import model_define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 445326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "445326"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(445326, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataloading.YelpDataLoader()\n",
    "data.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function encoder_factory.<locals>.encode at 0x72c21e9e4ea0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "WARNING:datasets.fingerprint:Parameter 'function'=<function encoder_factory.<locals>.encode at 0x72c21e9e4ea0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map:   0%|          | 0/11200 [00:00<?, ? examples/s]2025-02-19 11:27:44.112779: E tensorflow/core/util/util.cc:131] oneDNN supports DT_INT64 only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n",
      "Map: 100%|██████████| 11200/11200 [10:16<00:00, 18.17 examples/s]\n",
      "Map: 100%|██████████| 5600/5600 [05:34<00:00, 16.74 examples/s]\n",
      "Map: 100%|██████████| 760/760 [00:37<00:00, 20.20 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11200\n",
      "5600\n",
      "760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data.setup()\n",
    "print(len(data.train))\n",
    "print(len(data.val))\n",
    "print(len(data.test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_define import Model\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "MAX_EPOCHS = 15\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    dirpath=\"model\",\n",
    "    filename=\"yelp-sentiment-multilingual-{epoch:02d}-{val_loss:.3f}\",\n",
    "    save_top_k=3,\n",
    "    mode=\"min\")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=MAX_EPOCHS, \n",
    "                     callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name   | Type       | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | layers | Sequential | 492 K  | train\n",
      "----------------------------------------------\n",
      "492 K     Trainable params\n",
      "0         Non-trainable params\n",
      "492 K     Total params\n",
      "1.971     Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skullboxml/minor_project/venv/lib/python3.12/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 32. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 350/350 [00:11<00:00, 29.47it/s, v_num=0, train_loss_step=0.310, train_accuracy_step=0.875, val_loss_step=0.138, val_accuracy_step=0.938, val_loss_epoch=0.203, val_accuracy_epoch=0.921, train_loss_epoch=0.296, train_accuracy_epoch=0.876] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=15` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 350/350 [00:11<00:00, 29.46it/s, v_num=0, train_loss_step=0.310, train_accuracy_step=0.875, val_loss_step=0.138, val_accuracy_step=0.938, val_loss_epoch=0.203, val_accuracy_epoch=0.921, train_loss_epoch=0.296, train_accuracy_epoch=0.876]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data.train_dataloader(), data.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skullboxml/minor_project/venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at /home/skullboxml/minor_project/model/yelp-sentiment-multilingual-epoch=07-val_loss=0.196.ckpt\n",
      "Loaded model weights from the checkpoint at /home/skullboxml/minor_project/model/yelp-sentiment-multilingual-epoch=07-val_loss=0.196.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 24/24 [00:00<00:00, 50.10it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skullboxml/minor_project/venv/lib/python3.12/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 24. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9013158082962036     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.25619542598724365    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9013158082962036    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.25619542598724365   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.25619542598724365, 'test_accuracy': 0.9013158082962036}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(dataloaders=data.test_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = Model.load_from_checkpoint(checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import PrettyPrinter\n",
    "pp = PrettyPrinter()\n",
    "\n",
    "def predict(text: List[str]):\n",
    "    embeddings = torch.Tensor(dataloading.embed_text(text))\n",
    "    logits = best_model(embeddings)\n",
    "    preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n",
    "    scores = torch.softmax(logits, dim=1).detach().cpu().numpy()\n",
    "\n",
    "    results = []\n",
    "    for t, best_index, score_pair in zip(text, preds, scores):\n",
    "        results.append({\n",
    "            \"text\": t,\n",
    "            \"label\": \"positive\" if best_index == 1 else \"negative\",\n",
    "            \"score\": score_pair[best_index]\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'positive',\n",
      "  'score': np.float32(0.73088825),\n",
      "  'text': 'Like any Barnes & Noble, it has a nice comfy cafe, and a large '\n",
      "          'selection of books. The staff is very friendly and helpful. They '\n",
      "          'stock a decent selection, and the prices are pretty reasonable.'},\n",
      " {'label': 'positive',\n",
      "  'score': np.float32(0.96647483),\n",
      "  'text': 'Wie jedes Barnes & Noble hat es ein nettes, gemütliches Café und '\n",
      "          'eine große Auswahl an Büchern. Das Personal ist sehr freundlich und '\n",
      "          'hilfsbereit. Sie haben eine anständige Auswahl und die Preise sind '\n",
      "          'ziemlich vernünftig.'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8815/957889146.py:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  embeddings = torch.Tensor(dataloading.embed_text(text))\n"
     ]
    }
   ],
   "source": [
    "english_text = \"Like any Barnes & Noble, it has a nice comfy cafe, and a large selection of books. The staff is very friendly and helpful. They stock a decent selection, and the prices are pretty reasonable.\"\n",
    "\n",
    "german_translation = \"Wie jedes Barnes & Noble hat es ein nettes, gemütliches Café und eine große Auswahl an Büchern. Das Personal ist sehr freundlich und hilfsbereit. Sie haben eine anständige Auswahl und die Preise sind ziemlich vernünftig.\"\n",
    "\n",
    "pp.pprint(predict([english_text, german_translation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'negative',\n",
      "  'score': np.float32(0.98972845),\n",
      "  'text': 'The inside of the Restaurant was not clean at all. And we also did '\n",
      "          'not like their lighting arrangement. Too dark.'},\n",
      " {'label': 'negative',\n",
      "  'score': np.float32(0.75943637),\n",
      "  'text': \"L'interno del Ristorante non era affatto pulito. E non ci piaceva \"\n",
      "          'nemmeno la loro disposizione delle luci. Troppo scuro.'},\n",
      " {'label': 'positive',\n",
      "  'score': np.float32(0.90973556),\n",
      "  'text': 'Ravintolan sisäpuoli ei ollut ollenkaan puhdas. Ja emme myöskään '\n",
      "          'pitäneet heidän valaistusjärjestelystä. Liian pimeä.'}]\n"
     ]
    }
   ],
   "source": [
    "english_text = \"The inside of the Restaurant was not clean at all. And we also did not like their lighting arrangement. Too dark.\"\n",
    "\n",
    "italian_translation = \"L'interno del Ristorante non era affatto pulito. E non ci piaceva nemmeno la loro disposizione delle luci. Troppo scuro.\"\n",
    "\n",
    "finnish_translation = \"Ravintolan sisäpuoli ei ollut ollenkaan puhdas. Ja emme myöskään pitäneet heidän valaistusjärjestelystä. Liian pimeä.\"\n",
    "\n",
    "pp.pprint(predict([english_text, italian_translation, finnish_translation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'negative',\n",
      "  'score': np.float32(0.93024975),\n",
      "  'text': 'The inside of the Restaurant was not clean at all. And we also did '\n",
      "          'not like their lighting arrangement. Too dark.'},\n",
      " {'label': 'negative',\n",
      "  'score': np.float32(0.87560916),\n",
      "  'text': 'restaurant parishubramga ledu mariyu aa deepalu nachaledu'},\n",
      " {'label': 'positive',\n",
      "  'score': np.float32(0.9815802),\n",
      "  'text': 'Ravintolan sisäpuoli ei ollut ollenkaan puhdas. Ja emme myöskään '\n",
      "          'pitäneet heidän valaistusjärjestelystä. Liian pimeä.'}]\n"
     ]
    }
   ],
   "source": [
    "english_text = \"The inside of the Restaurant was not clean at all. And we also did not like their lighting arrangement. Too dark.\"\n",
    "\n",
    "telugu_translation = \"restaurant parishubramga ledu mariyu aa deepalu nachaledu\"\n",
    "\n",
    "finnish_translation = \"Ravintolan sisäpuoli ei ollut ollenkaan puhdas. Ja emme myöskään pitäneet heidän valaistusjärjestelystä. Liian pimeä.\"\n",
    "\n",
    "pp.pprint(predict([english_text, telugu_translation, finnish_translation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This food is too dandelion!\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from googletrans import Translator\n",
    "\n",
    "async def translate_text():\n",
    "    translator = Translator()\n",
    "    original_text = \"این غذا خیلی شوره!\"\n",
    "    translation = await translator.translate(original_text, dest=\"en\")\n",
    "    print(translation.text) \n",
    "\n",
    "await translate_text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "translator = Translator()\n",
    "text = input()\n",
    "\n",
    "detected_lang = translator.detect(text)\n",
    "if detected_lang == \"en\":\n",
    "    pp.pprint(predict([text]))\n",
    "else:\n",
    "    translated_text = translate_text(text)\n",
    "    pp.pprint(predict([translated_text]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
